\documentclass[12pt]{article}
\usepackage{url,graphicx,tabularx,array}
\usepackage[margin=1in]{geometry}
\setlength{\parskip}{1ex} %--skip lines between paragraphs
\setlength{\parindent}{0pt} %--don't indent paragraphs

\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{ amssymb }
\usepackage{ latexsym }
\usepackage{ amsmath }
\usepackage{ amsthm }
%-- Commands for header
\renewcommand{\title}[1]{\textbf{#1}\\}
\renewcommand{\line}{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}\hline\\\end{tabularx}\\[-0.5cm]}
\newcommand{\leftright}[2]{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}#1%
& #2\\\end{tabularx}\\[-0.5cm]}

\newtheorem{defn}{Definition}[section]
\newtheorem{conjecture}{conjecture}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{question}{Question}[section]
\newtheorem{proposition}{Proposition}[section]


%\linespread{2} %-- Uncomment for Double Space
\begin{document}

\title{Homework 3: CMPS 242}
\line
\leftright{\today}{Bryan Matsuo (bmatsuo@soe.ucsc.edu) \& John St. John (jstjohn@soe.ucsc.edu)} %-- left and right positions in the header
\begin{enumerate}
\item \textbf{Logistic regression:}
We need to show that $\ln \frac{p(1|x,w)}{p(0|x.w)}=w\cdot x$. We are given that in this case $p(1|w,x) = \frac{e^{w\cdot x}}{1+e^{w \cdot x}}$

\begin{eqnarray*}
\ln \frac{p(1|x,w)}{p(0|x.w)} \\
= \ln p(1|w,x) - \ln p(0|w,x) \\
=  \ln \frac{e^{w\cdot x}}{1+e^{w \cdot x}} - \ln \left[ 1-\frac{e^{w\cdot x}}{1+e^{w \cdot x}}\right] \\
=\ln e^{w\cdot x} - \ln \left[ 1 + e^{w\cdot x} \right] -\ln \left[ 1-\frac{e^{w\cdot x}}{1+e^{w \cdot x}}\right] \\
=\ln e^{w\cdot x} - \ln \left[ 1 + e^{w\cdot x} \right] -\ln \left[ \frac{e^{w\cdot x}}{e^{w\cdot x}}-\frac{e^{w\cdot x}}{1+e^{w \cdot x}}\right] \\
=\ln e^{w\cdot x} - \ln \left[ 1 + e^{w\cdot x} \right] -\ln \left[ \frac{e^{w\cdot x}}{e^{w\cdot x}+e^{2\cdot w\cdot x}}+ \frac{e^{2\cdot w\cdot x}}{e^{w\cdot x}+e^{2\cdot w\cdot x}}-\frac{e^{2\cdot w\cdot x}}{e^{w\cdot x}+e^{2\cdot w\cdot x}}\right] \\
=\ln e^{w\cdot x} - \ln \left[ 1 + e^{w\cdot x} \right] -\ln \left[ \frac{e^{w\cdot x}}{e^{w\cdot x}+e^{2\cdot w\cdot x}}\right] \\
=\ln e^{w\cdot x} - \ln \left[ 1 + e^{w\cdot x} \right] -\ln e^{w\cdot x} + \ln \left[ e^{w\cdot x}+e^{2\cdot w\cdot x} \right] \\
= - \ln \left[ 1 + e^{w\cdot x} \right] +  \ln \left[ e^{w\cdot x}+e^{2\cdot w\cdot x} \right] \\
= - \ln \left[ 1 + e^{w\cdot x} \right] + \ln \left[ e^{w\cdot x}\left(1+e^{\cdot w\cdot x}\right) \right]\\
= - \ln \left[ 1 + e^{w\cdot x} \right] + \ln e^{w\cdot x} + \ln \left[1+e^{\cdot w\cdot x}\right] \\
=\ln e^{w\cdot x}\\
= w\cdot x
\end{eqnarray*}


\item \textbf{Weka Experiments: }

\begin{enumerate}
\item %a
The weighted average precision and recall are shown in table~\ref{tab:parta}. Although it is tempting to say that the Nearest Neighbor method is the best, there is a reason it scored perfectly while the others did not. Nearest Neighbor includes each datapoint in its training set, along with the correct label, so when you test on the training set it simply spits the correct labels back out at you. Taking that into consideration it is impossible to say which method was the best. We'll need to evaluate the methods on a different set of data than was used to train to compare the techniques.


\begin{table}[htdp]
\caption{Precision, recall, and F-measure on the training data}
\begin{center}
\begin{tabular}{l||c|c|c|}
               & Nearest & Naive & Logistic  \\
               & Neighbor &  Bayes & Regression  \\
               \hline
Tested Negative Precision & 1 & 0.803 & 0.799 \\
Tested Negative Recall &1  &  0.842 & 0.89 \\
Tested Negative F-Measure &  1& 0.822 & 0.842 \\
Tested Positive Precision & 1 & 0.676 & 0.739 \\
Tested Positive Recall &1  & 0.616 & 0.582 \\
Tested Positive F-Measure &  1 & 0.645 & 0.651 \\
Weighted Avg. Precision & 1 & 0.759 & 0.778\\
Weighted Avg. Recall & 1 & 0.763 & 0.783 \\
Weighted Avg. F-Measure & 1  & 0.76  &  0.775 \\
\end{tabular}
\end{center}
\label{tab:parta}
\end{table}%


\item %b
Given the following output from Weka, the weight vecotor \[w=\left\{ -0.1232, -0.0352,   0.0133,  -0.0006,  0.0012, -0.0897,  -0.9452, -0.0149 \right\}\] and the bias is $b=8.4047$. As can be seen in the following python computation this does indeed nearly come to 0.

{\bf Python Computation of one point with probability near 50/50}
\begin{verbatim}
In [2]: x=[5,144,82,26,285,32,0.452,58]
In [5]: w=[-0.1232, -0.0352, 0.0133, -0.0006, 0.0012, -0.0897, -0.9452, -0.0149]
In [6]: bias= 8.4047
In [16]: sum([(a*b) for (a,b) in zip(w,x)]) + bias
Out[16]: -0.024930400000000574
\end{verbatim}

{\bf Weka's Weight Vector Output}
\begin{verbatim}
                       Class
Variable     tested_negative
============================
preg                 -0.1232
plas                 -0.0352
pres                  0.0133
skin                 -0.0006
insu                  0.0012
mass                 -0.0897
pedi                 -0.9452
age                  -0.0149
Intercept             8.4047
\end{verbatim}



\item %c
This time since the test set was different from the training set, we can start to get a much more realistic idea of the accuracy of each method, the accuracy results from this run are shown in table~\ref{tab:partc}. Now that the training and test set are different we can see that the Nearest Neighbor technique actually performs the worst on our data while Naive Bayes appears to be the best for a medical setting. Since Diabetes is can lead to very expensive procedures if not treated in a timely fashion, the most important thing is that when you tell someone they are not at risk, they really aren't. If you tell someone that they are at risk but they turn out not to be it isn't as big of a deal. Thus for your negative predictions you want high precision, but more importantly for the positives you want high recall. Naive Bayes has both the highest recall for positives and highest precision for negatives.

\begin{table}[htdp]
\caption{Precision, recall, and F-measure on 10-fold CV of the data}
\begin{center}
\begin{tabular}{l||c|c|c|}
               & Nearest & Naive & Logistic  \\
               & Neighbor &  Bayes & Regression  \\
               \hline
Tested Negative Precision & 0.759 & 0.802 & 0.793 \\
Tested Negative Recall & 0.794  & 0.763 & 0.88 \\
Tested Negative F-Measure & 0.776 & 0.823 & 0.834 \\
Tested Positive Precision & 0.58 & 0.759  & 0.718 \\
Tested Positive Recall & 0.53 & 0.612 & 0.571 \\
Tested Positive F-Measure &  0.554 & 0.643 & 0.636  \\
Weighted Avg. Precision & 0.696 & 0.759 & 0.767\\
Weighted Avg. Recall & 0.702  & 0.763 & 0.772 \\
Weighted Avg. F-Measure & 0.698   & 0.76  & 0.765  \\
\end{tabular}
\end{center}
\label{tab:partc}
\end{table}%

\item %d
\begin{enumerate}
\item
The normalize function takes each numeric  attribute and normalizes it relative to the corresponding attribute in the remainder of the samples. After normalization each attribute has some value between 0 and 1 inclusive. 
\item
The only method with different accuracy values after normalization was Naive Bayes. Naive Bayes is probably configured to model each attribute with a gaussian distribution. Now that the points are represented on $[0,1]$ it is possible that the tighter spacing of points lead to the classifier having a more difficult time differentiating the classes with some higher level combination of gaussians. 
\item %f

The weight vectors (shown below) appear to be more extreme after normalization than before normalization. The negative weights are more negative and the positives are more positive. This is probably needed to achieve adequate separation of the data now that it has been compacted into the [0,1] range.


{\bf Weight Vector before normalization:}
\begin{verbatim}
                       Class
Variable     tested_negative
============================
preg                 -0.1232
plas                 -0.0352
pres                  0.0133
skin                 -0.0006
insu                  0.0012
mass                 -0.0897
pedi                 -0.9452
age                  -0.0149
Intercept             8.4047
\end{verbatim}

{\bf Weight Vector after normalization:}
\begin{verbatim}
                       Class
Variable     tested_negative
============================
preg                 -2.0941
plas                 -6.9976
pres                  1.6221
skin                 -0.0613
insu                  1.0082
mass                 -6.0189
pedi                 -2.2136
age                  -0.8921
Intercept             8.0187

\end{verbatim}

\end{enumerate}

\item %e
As can be clearly seen, to 4 significant digits, the weight vector doesn't change between the default value of R=$1\times10^{-8}$ and R=0.0. When R is increased to 0.1 though the weight vector does change slightly, as does the intercept. The accuracy for all values of R I have tried is the same.



{\bf R=0.1}
\begin{verbatim}
                       Class
Variable     tested_negative
============================
preg                 -2.0877
plas                 -6.9767
pres                  1.6139
skin                 -0.0602
insu                  0.9985
mass                 -6.0008
pedi                 -2.2076
age                   -0.894
Intercept             7.9997
\end{verbatim}
{\bf R=0.0}
\begin{verbatim}
                       Class
Variable     tested_negative
============================
preg                 -2.0941
plas                 -6.9976
pres                  1.6221
skin                 -0.0613
insu                  1.0082
mass                 -6.0189
pedi                 -2.2136
age                  -0.8921
Intercept             8.0187
\end{verbatim}

\item %f
I would expect 3NN to do better than Nearest Neighbor. Averaging over a few points can take care of some improperly labeled points. Indeed it appears from the results below that for many measures of accuracy, 3NN is an improvement over 1NN and 5NN is even a little better than 3NN.

{\bf 1NN}
\begin{verbatim}
=== Detailed Accuracy By Class ===
               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area 
                 0.794     0.47       0.759     0.794     0.776      0.662  
                 0.53      0.206      0.58      0.53      0.554      0.662    
Weighted Avg.    0.702     0.378      0.696     0.702     0.698      0.662
\end{verbatim}

{\bf 3NN}
\begin{verbatim}
=== Detailed Accuracy By Class ===
               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area 
                 0.82      0.448      0.774     0.82      0.796      0.742   
                 0.552     0.18       0.622     0.552     0.585      0.742   
Weighted Avg.    0.727     0.354      0.721     0.727     0.722      0.742
\end{verbatim}

{\bf 5NN}
\begin{verbatim}
=== Detailed Accuracy By Class ===
               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  
                 0.836     0.463      0.771     0.836     0.802      0.766    
                 0.537     0.164      0.637     0.537     0.583      0.766    
Weighted Avg.    0.732     0.358      0.724     0.732     0.726      0.766
\end{verbatim}

\item %g

\item %h

Nearest Neighbor suffered a fairly significant accuracy hit, the weighted average F-measure went down from 0.698 to 0.595. With logistic regression the weighted average F-measure went from 0.765 down to  0.742. While Naive Bayes remained relatively unchanged right around 0.76 for its F-measure. So in order NN suffers the most from randomized attributes, logistic regression suffers less and naive bayes suffers the least.

{\bf Nearest Neighbor}
\begin{verbatim}
=== Detailed Accuracy By Class ===
               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area 
                 0.746     0.657      0.679     0.746     0.711      0.545  
                 0.343     0.254      0.42      0.343     0.378      0.545   
Weighted Avg.    0.605     0.516      0.589     0.605     0.595      0.545
\end{verbatim}

{\bf Naive Bayes}
\begin{verbatim}
=== Detailed Accuracy By Class ===
               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area 
                 0.858     0.403      0.799     0.858     0.827      0.819 
                 0.597     0.142      0.693     0.597     0.641      0.819 
Weighted Avg.    0.767     0.312      0.762     0.767     0.762      0.819
\end{verbatim}

{\bf Logistic Regression}
\begin{verbatim}
=== Detailed Accuracy By Class ===
               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area
                 0.856     0.451      0.78      0.856     0.816      0.825  
                 0.549     0.144      0.671     0.549     0.604      0.825 
Weighted Avg.    0.749     0.344      0.742     0.749     0.742      0.825
\end{verbatim}



\end{enumerate}
\end{enumerate}

\end{document}
