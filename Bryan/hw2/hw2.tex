\documentclass[11pt]{article}

\usepackage{fullpage,latexsym,amsthm,amsmath,algorithmic,graphicx,subfigure,color,verbatim,epsfig,fancyhdr,multicol}
%\usepackage[normalmargins]{savetrees}

% Define new commands.
\newtheorem{defn}{Definition}%[section]
\newtheorem{conjecture}{conjecture}%[section]
\newtheorem{lemma}{Lemma}%[section]
\newtheorem{corollary}{Corollary}%[section]
\newtheorem{question}{Question}%[section]
\newtheorem{proposition}{Proposition}%[section]

\newcommand{\thmref}[1]{\ref{theorem: #1}}
\newcommand{\lemref}[1]{\ref{lemma: #1}}
\newcommand{\propref}[1]{\ref{proposition: #1}}
\newcommand{\conjref}[1]{\ref{conjecture: #1}}
\newcommand{\questref}[1]{\ref{question: #1}}
\newcommand{\eqnref}[1]{\ref{equation: #1}}
\newcommand{\figref}[1]{\ref{figure: #1}}
\newcommand{\algref}[1]{\ref{algorithm: #1}}
\newcommand{\tabref}[1]{\ref{table: #1}}

%\newcommand{\abcitem}{\item[\alph({alphaenum})]\stepcounter{alphaenum}}
\newenvironment{abclist}{
\begin{list}{(\alph{alphaenum}) ~}
    {\newcounter{alphaenum} \setcounter{alphaenum}{1} \usecounter{alphaenum} }
    }
    {
\end{list}
}


\begin{document}

% Make the document title.
\title{CMPS 242 - Spring 2011 - Homework assignment \#2}
\author{ {Bryan Matsuo}\\
{Department of Computer Science, UCSC}}
%\date{\today}
\maketitle

%\begin{multicols}{2}
\section*{1 Number Crunching\dots}

\section*{2 Naive Bayes}

\section*{3 Bayesian Decision Theory}

In this problem we are classifying automobiles.
Given an instance.
We can predict {\bf B}us, {\bf C}ar, or a {\bf T}ruck.
We may also choose to {\bf A}bstain (give no prediction).
The cost matrix for the decisions is
\begin{center}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        loss &
        \multicolumn{4}{|c|}{prediction} \\
        \multicolumn{1}{|r|} {class} & B & C & T & A \\
        \hline
        B & 0 & 1 & 1 & $a$ \\
        C & 1 & 0 & 1 & $a$ \\
        T & 1 & 1 & 0 & $a$ \\
        \hline
    \end{tabular}
\end{center}
where $a$ is a cost (or loss) incurred from abstaining from a prediction.
We will denote the loss of a prediction $\hat y$ given a true label $y$ as
$L(\hat{y} \mid y)$. For example, $L(B|C)=1, L(T|T) = 0 \text{ and } L(A|C)=a$.

\begin{abclist}
\item Assume that for an arbitrary instance $P(B) = 0.5, P(C) = 0.25, \text{ and } P(T) = 0.25$.
    The risk $R(\hat y)$ of any prediction $\hat y$ is 
    \begin{equation}
        R(\hat y) = E_y[L(\hat{y} \mid y)] = \sum_{y \in \left\{ B,C,T \right\}} P(y) L(\hat{y} \mid y)
        \label{equation: risk}
    \end{equation}.

    Equation \eqnref{risk} allows us to calculate the risk associated with each prediction (or lack there of)
    \begin{center}
        \begin{tabular}{|l|c|}
            \hline
            class & risk \\
        \hline
        B & 0.5 \\
            C & 0.75 \\
            T & 0.75 \\
            A & $a$ \\
            \hline
        \end{tabular}
    \end{center}.

\item A risk averse predictor would predict {\bf B} for any new instance if $a > 0.5 = R(B)$.
    They could predict {\bf B} or {\bf A}bstain with equal expectation for cost if $a = 0.5$.
    As $a$ tends to zero, the best predition for the risk averse predictor becomes to always abstain from giving a prediction, for which the cost tests to zero.
\end{abclist}


\section*{4 Expectation of a product}
    First consider the following proposition.
    \begin{proposition}[Conditional expection of a product.]
        If $V$ and $W$ are random variables and $Z=VW$, then $E[Z|V=v,W=w]=\int_{-\infty}^{\infty}p(Z=z \mid V=v,W=w)dz = wv$.
        \label{proposition: conditional expectation}
    \end{proposition}
    This is true in either the continuous or discrete case.
    It is also a very straight forward and intuitive idea.
    To actually show it would require more in depth arguments involving the hairyness of the Dirac delta function for continuous variables.
    So here it is taken as fact.

    We can now discuss the main result of this problem.
\begin{lemma}[Product of idependent variables]
    Let $V$ and $W$ be independent random variables. Then $E[VW] = E[V]E[W]$.
\end{lemma}
\begin{proof}
    Assume $V$ and $W$ and independent random variables (numbers) and let $Z=VW$.
    %The joint probability density of $V$ and $Z$ is given as
    %\begin{equation}
    %p(V=v,Z=z) = p(V=v)p(Z=z|V=v) = p(V=v)p\bigg(W=\frac{z}{v}\bigg)
    %\end{equation}
    %Then the probability density of $Z$, $p(Z=z)$ is
    %\begin{eqnarray}
    %p(Z=z) &=& \int_{-\infty}^{\infty}p(V=v,Z=z)dv \\
    %&=& \int_{-\infty}^{\infty}p(V=v)p\bigg(W=\frac{z}{v}\bigg)dv
    %\label{equation: indep_prod_density}
    %\end{eqnarray}
    %The expected value of $Z$ is then
    %\begin{eqnarray}
    %E[Z] &=& \int_{-\infty}^{\infty} zp(Z=z) dz \nonumber \\
    %&=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}z p(V=\nu)p\bigg(W=\frac{z}{\nu}\bigg)d\nu dz \\
    %\label{equation: full_expected_indep_prod}
    %\end{eqnarray}
    %Making the change of variables $\nu = v, z = vw$, we get
    %\begin{eqnarray}
    %E[Z] &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
    %vwp(V=v)p(W=w) (v \text{ \color{red}{\dots shit}}) dv dw \\
    %&=& \int_{-\infty}^{\infty} wp(W=w) \int_{-\infty}^{\infty} v^2 p(V=v) dv dw \\
    %&=& E[W]E[V^2]
    %\end{eqnarray}
    %That extra $v$ term that popped out of the determinant of the Jacobian really throws things off.
    %I notice now that I never actually used the fact that $V$ and $W$ are independent.
    %This could be the key flaw in the proof, but I can't see where to apply it.
    %{\color{blue}Alternatively}, 
    
    %the conditional probability mass of $Z$ (discrete case) can be viewed as a simple indicator function.
    %\begin{equation}
    %p(Z=z|V=v,W=w) = {\bf 1}_{z=vw}
    %\label{equation: p z discrete}
    %\end{equation}
    %This should be obvious.
    %When the values of $V$ and $W$ are known there is no randomness left in $Z$.
    %Furthermore, it should be clear that the expected value of $Z$ given $V=v$ and $W=w$, $E[Z|V=v,W=w]$ is equal to $vw$.
    %
    %In the continuous case, the conditional density takes the form of a Dirac delta function shifted so all of $Z$'s density is centered over $vw$.
    %\begin{equation}
    %p(Z=z|V=v,W=w) = \delta(z - vw)
    %\label{equation: p z}
    %\end{equation}
    %More explicitly $p(Z=z|V=v,W=w)$ is equal to 0 for all $z \neq vw$, but  \[\int_{-\infty}^{\infty}p(Z=z|V=v,W=w)dz = \int_{-\infty}^{\infty}\delta(z-vw)dz = 1\].
    %The final observation about the delta function that we will make is that 
    %\begin{equation}
    %\int_{-\infty}^{\infty} f(z)\delta(z-vw)dz = f(vw)
    %\label{equation: delta func eval}
    %\end{equation}.
    %In particular, $E[Z] = \int_{-\infty}^{\infty} z\delta(z-vw)dz = vw$.
    
    The joint probability density of $V$, $W$, and $Z$ can be expanded using the chain rule of probability and our assumption of independence between $V$ and $W$.
    \begin{eqnarray}
        p(V=v, W=w, Z=z) &=& p(V=v, W=w)p(Z=z \mid V=v,W=w) \nonumber \\
            &=& p(V=v)p(W=w)p(Z=z \mid V=v,W=w)
            \label{equation: joint density}
    \end{eqnarray}.
    The probability density of $Z$ can then be marginalized from the join density.
    \begin{eqnarray}
        p(Z=z) &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(V=v, W=w, Z=z) dwdv \nonumber \\
        &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(V=v)p(W=w)p(Z=z \mid V=v,W=w) dwdv
        \label{equation: p z}
    \end{eqnarray}
    With proposition equation \eqnref{p z} and \propref{conditional expectation} we can calculate the expected value of $Z=VW$.
    \begin{eqnarray}
        E[Z] &=& \int_{-\infty}^{\infty} zp(Z=z)dz \nonumber \\
        &=& \int_{-\infty}^{\infty} z \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(V=v,W=w,Z=z) dwdvdz \nonumber \\
        &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} zp(V=v)p(W=w)p(Z=z \mid V=v,W=w) dwdvdz \nonumber \\
        &=& \int_{-\infty}^{\infty} p(V=v) \int_{-\infty}^{\infty} p(W=w) \int_{-\infty}^{\infty} zp(Z=z \mid V=v,W=w) dzdwdv \nonumber \\
        &=& \int_{-\infty}^{\infty} vp(V=v) \int_{-\infty}^{\infty} wp(W=w) dw dv = E[V]E[W]
        \label{equation: z_mean}
    \end{eqnarray}
    The order of integration is changed, which should be applicable in the discrete and continuous case\dots
\end{proof}

Let $R$ be a Bernoulli random variable equal to 1 with probablity 0.5, and equal to zero with probability 0.5.
Now, let $S=R$ and $T=RS$.
Note that $T$ has the same value as $R^2$.
Also, $R^2$ has the same value as $R$, since $R$ can only be 0 or 1.
So the random variable $T$ is identical to $R$.
So $E[T]=E[RS]=E[R^2]=E[R]=0.5$. This is not equal to $E[R]E[S]=E[R]^2=0.25$.




% Add a bibliography.
%\bibliographystyle{apalike}
%\bibliography{project}

%\end{multicols}

\end{document}
