\documentclass[12pt]{article}
\usepackage{url,graphicx,tabularx,array,color}
\usepackage[margin=1in]{geometry}
\setlength{\parskip}{1ex} %--skip lines between paragraphs
\setlength{\parindent}{0pt} %--don't indent paragraphs

\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{ amssymb }
\usepackage{ latexsym }
\usepackage{ amsmath }
\usepackage{ amsthm }
\usepackage{Sweave}
%-- Commands for header
\renewcommand{\title}[1]{\textbf{#1}\\}
\renewcommand{\line}{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}\hline\\\end{tabularx}\\[-0.5cm]}
\newcommand{\leftright}[2]{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}#1%
& #2\\\end{tabularx}\\[-0.5cm]}

\DeclareMathOperator*{\argmax}{arg\,max}

\newtheorem{defn}{Definition}[section]
\newtheorem{conjecture}{conjecture}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{question}{Question}[section]
\newtheorem{proposition}{Proposition}[section]

\newcommand{\thmref}[1]{\ref{theorem: #1}}
\newcommand{\lemref}[1]{\ref{lemma: #1}}
\newcommand{\propref}[1]{\ref{proposition: #1}}
\newcommand{\conjref}[1]{\ref{conjecture: #1}}
\newcommand{\questref}[1]{\ref{question: #1}}
\newcommand{\eqnref}[1]{\ref{equation: #1}}
\newcommand{\figref}[1]{\ref{figure: #1}}
\newcommand{\algref}[1]{\ref{algorithm: #1}}
\newcommand{\tabref}[1]{\ref{table: #1}}

\newcounter{alphaenum}
\newenvironment{abclist}
    { \begin{list}{(\alph{alphaenum}) ~} { \setcounter{alphaenum}{1} \usecounter{alphaenum} } }
    { \end{list} }

%\linespread{2} %-- Uncomment for Double Space
\begin{document}

\title{Homework 2: CMPS 242}
\line
\leftright{\today}{Bryan Matsuo (bmatsuo@soe.ucsc.edu) \& John St. John (jstjohn@soe.ucsc.edu)} %-- left and right positions in the header

\section{Number Crunching\dots}
Note: The probabilities in this problem were calculated with a python script. It should be attached at the end of the assignment.

This problem focuses on a discriminative model where a hypothesis $h$ is chosen from a prior and instances are labeled according to the probabilities given by $h$.
The instance space in this problem is $\left\{ a,b,c \right\}$, the label space is $\left\{ +,- \right\}$, and the hypothesis class $\mathcal{H}$ consists of hypothesis $h_1,h_2,h_3$.
The hypotheses of $H$ assign probabilities of instances being $+$ according to the following table.
\begin{center}
    \begin{tabular}{c c c c}
            & $h_1$ & $h_2$ & $h_3$ \\
        $a$ & 0.8 & 0.25 & 0.5 \\
        $b$ & 0.8 & 0.75 & 0.5 \\
        $c$ & 0.2 & 0.75 & 0.5
    \end{tabular}
\end{center}

The hypotheses of $\mathcal{H}$ have prior probabilities $p(h_1) = \frac{1}{2}$, $p(h_2) = \frac{4}{10}$, and $p(h_3) = \frac{1}{10}$.

For parts (a)-(d), assume that we have the priors described above, and then see the sample $(a,+),(b,-),(c,+)$.

\begin{abclist}
\item For a point $x\in \left\{a,b,c\right\}$ we have $p(x=+) = \sum_{h_i}p(x=+|h_i)$.
    This produces the following prior probabilities of a point being labeled $+$.
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            Point & Prob. of $+$ \\
            \hline
            $a$ & 0.55 \\
            $b$ & 0.75 \\
            $c$ & 0.45 \\
            \hline
        \end{tabular}
    \end{center}
\item The likelihood of a sample $(x_1,y_1)\dots(x_n,y_n)$ under hypothesis $h_i$ is \[p(x_1 = y_1, \dots, x_n = y_n | h_i) = \prod_{j=1}^{n}p(x_j = y_j | h_i)\] assuming the samples labels are independent.
    So we can calculate the following table containing the likelihood of the sample $(a,+)(b,-)(c,+)$ under each hypotheses.
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            Hypothesis & Likelihood of sample \\
            \hline
            $h_1$ & 0.032 \\
            $h_2$ & 0.046 \\
            $h_3$ & 0.125 \\
            \hline
        \end{tabular}
    \end{center}
    The sample is most likely to occur under hypothesis $h_3$ and thus it is the maximum likelihood hypothesis given the sample.
\item The posterior probability of a hypothesis $h_i$ after seeing a sample $(x_1,y_1)\dots(x_n,y_n)$ is \[ p(h_i|x_1=y_1,\dots,x_n=y_n) = \frac{p(h_i)p(x_1=y_1,\dots,x_n=y_n | h_i)}{p(x_1=y_1,\dots,x_n=y_n)} \] using Bayes' rule.
    The prior probabilities $p(h_i)$ are given, and the conditional probabilities $p(x_1=y_1,\dots,x_n=y_n | h_i)$ are the likelihoods calculated in part (b).
    The normalizing constant $p(x_1=y_1,\dots,x_n=y_n)$ can be calculated as the product of prior probabilities $p(x_j=y_j)$ for $1 \leq j \leq n$.
    That is 
    \begin{eqnarray}
        p(x_1=y_1,\dots,x_n=y_n) &=& \prod_{i=1}^{n} p(x_j=y_j) \\
        &=& \prod_{j=1}^{n} \sum_{h_i}p(x_j=y_j|h_i)
    \end{eqnarray}
    This product of sums computation can be avoided by normalizing the unscaled posterior probability by the sum of unscaled posterior probabilities for all hypotheses.
    This ensures they sum to one.
    For computing the maximum a posteriori hypothesis, normalization is unnecessary,
    but we do it in our python code because it is easy, runtime is not a concern, and it simplifies the solution to the next part of this problem.
    
    The resulting a posteriori distribution over hypotheses is summarized in the following table.
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            Hypothesis & Posterior probability \\
            \hline
            $h_1$ & 0.3386 \\
            $h_2$ & 0.3968 \\
            $h_3$ & 0.2646 \\
            \hline
        \end{tabular}
    \end{center}
    As the table shows, the maximum a posteriori hypothesis (the hypothesis with the greatest posterior probability) is $h_3$.
\item The mean posterior probability of an example $(x,y)$ after seeing sample $(x_1,y_1)\dots(x_n,y_n)$ can be calculated as \[ E_{h_i}[p(x=y \mid h_i) \mid x_1=y_1\dots x_n=y_n] = \sum_{h_i} p(h_i|x_1 = y_1 \dots x_n = y_n)p(x=y|h_i) \]
    The mean posterior probability of label $+$ for each point is given in the following table.
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            Point & Mean posterior probability of $+$ \\
            \hline
            $a$ & 0.5024 \\
            $b$ & 0.7008 \\
            $c$ & 0.4976 \\
            \hline
        \end{tabular}
    \end{center}
\item
    In this part of the problem
    we assume that instead of seeing the same from the first parts of this problem,
    we see a sample twice the size: $(a,+),(b,-),(c,+),(a,+),(b,-),(c,+)$.

    This of course will not change the prior probabilities of labels computed in part (a).
    Those probabilities are not conditioned on any observations.
    However, the likelihood of the sample, conditioned on specific hypothesis or not, will change.
    In fact, the sample's likelihoods here are the square of what they were in the previous sections.
    This is because the new sample of part (e) consists of exactly two copies of every example in the previous parts' sample.
    Because the likelihoods here are all squared, the maximum likelihood hypothesis given the sample, $h_3$ is the same as in part (b).
    The following table shows the computed likelihoods.
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            Hypothesis & Likelihood of sample \\
            \hline
            $h_1$ & 0.001023 \\
            $h_2$ & 0.002197 \\
            $h_3$ & 0.015625 \\
            \hline
        \end{tabular}
    \end{center}

    Because the likelihoods of the data are different than before,
    all the posterior calculations and predictions from previous parts are no longer the same.
    While the squaring of likelihoods is monotonic, it is not enough to guarantee that the MAP hypothesis will be the same.
    And as it turns out, the MAP hypothesis is now $h_3$, like the maximum likelihood hypothesis and unlike the MAP hypothesis from part (c).
    The following table summarizes the posterior probabilities over hypotheses.
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            Hypothesis & Posterior probability \\
            \hline
            $h_1$ & 0.173359 \\
            $h_2$ & 0.297591 \\
            $h_3$ & 0.529050 \\
            \hline
        \end{tabular}
    \end{center}

    Finally, because our posteriors are all different there is no reason to assume that the mean posterior probabilities of $+$ for any instance will be the same.
    This is shown in the following table.
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            Point & Mean posterior probability of $+$ \\
            \hline
            $a$ & 0.47761 \\
            $b$ & 0.62640 \\
            $c$ & 0.52238 \\
            \hline
        \end{tabular}
    \end{center}
    It is interesting that the mean posterior probability of $a=+$ is lower than in part (d), even though another positive $a$ instance was seen here.
    A possible explanation of this would be that the hypothesis class is not diverse enough to fit the data very well.
    Although, in this example where the label instances are not very random looking, a hypothesis class with more diverse hypotheses might run the risk of overfitting the data.
\end{abclist}

\section{Naive Bayes}

\begin{Schunk}
\begin{Sinput}
> dat
\end{Sinput}
\begin{Soutput}
  GPA AP class
1 4.0  y     H
2 3.7  y     H
3 2.5  n     H
4 3.8  n     N
5 3.3  y     N
6 3.0  y     N
7 3.0  n     N
8 2.7  n     N
9 2.2  n     N
\end{Soutput}

%We generate instances in fields using the class.
In with Naive Bayes we assume that instance features are uncorrelated.
When a new instance $x$ is seen with AP status $x_{AP}$ and GPA $x_{GPA}$ we calculate probability
\[p(C=c,AP=x_{AP},GPA=x_{GPA}) = p(C=c)p(AP=x_{AP}|C=c)p(GPA=x_{GPA}|C=c)\]
for each class $c$ and predict with the class which maximizes this probability.
The independence assumption of Naive Bayes allows the factoring performed above.

As such, to compute the naive bayes prediction rule in this case we much compute conditional distibutions for each instance feature given either label.
We also must compute prior distribution over the instance labels.
So, that is $2 \times (\text{number of features}) + 1=5$ distributions that must be computed for this problem.

First, we compute distributions for the conditioned random variable $AP|C$.
Note that $p(AP=yes|C=H)$ and $p(AP=no|C=H)$ sum to one, but $p(AP=yes|C=H)$ and $p(AP=yes|C=N)$ are not required to sum to one (although they do in this example).
\begin{Schunk}
\begin{Sinput}
> n.n <- sum(dat$class == "N")
> n.h <- sum(dat$class == "H")
> n.ap.y.class.h <- sum(dat$class == "H" & dat$AP == "y")
> n.ap.y.class.n <- sum(dat$class == "N" & dat$AP == "y")
> print(p.ap.y_class.h <- n.ap.y.class.h/n.h)
\end{Sinput}
\begin{Soutput}
[1] 0.6666667
\end{Soutput}
\begin{Sinput}
> print(p.ap.y_class.n <- n.ap.y.class.n/n.n)
\end{Sinput}
\begin{Soutput}
[1] 0.3333333
\end{Soutput}
\end{Schunk}

We assume the conditional GPA score distribution is distributed normally.
That is $GPA|C=H \sim N(\mu_H,\sigma_H^2)$ and $GPA|C=N \sim N(\mu_N,\sigma_N^2)$.
The parameters for these gaussian distributions are estimated from the sample points using non-biased estimators.
\begin{Sinput}
> print(mean.gpa.by.class <- ddply(dat, .(class), colwise(mean, 
+     .(mean.GPA = GPA))))
\end{Sinput}
\begin{Soutput}
  class mean.GPA
1     H      3.4
2     N      3.0
\end{Soutput}
\begin{Sinput}
> print(sd.gpa.by.class <- ddply(dat, .(class), colwise(sd, .(sd.GPA = GPA))))
\end{Sinput}
\begin{Soutput}
  class    sd.GPA
1     H 0.7937254
2     N 0.5403702
\end{Soutput}
\end{Schunk}

Finally, we must estimate prior distribution over instance label $C$ and can just use empirical counts.
\begin{Schunk}
\begin{Sinput}
> print(pc.n <- n.n/(n.n + n.h))
\end{Sinput}
\begin{Soutput}
[1] 0.6666667
\end{Soutput}
\begin{Sinput}
> print(pc.h <- n.h/(n.n + n.h))
\end{Sinput}
\begin{Soutput}
[1] 0.3333333
\end{Soutput}
\end{Schunk}

As mentioned at the beginning of the problem, to determine which class we predict for a new instance $x$ we must compute
\[\argmax_{c\in\left\{ H,N \right\}} p(C=c)p(AP=x_{AP}|C=c)p(GPA=x_{GPA}|C=c)\].
Alternatively, we can solve for $x_{GPA}$ in inequalities
\begin{eqnarray*}
    p(AP=yes|C=H)p(GPA=x_{GPA}|C=H) &>& p(AP=yes|C=N)p(GPA=x_{GPA}|C=N) \\
    \text{and } p(AP=no|C=H)p(GPA=x_{GPA}|C=H) &>& p(AP=no|C=N)p(GPA=x_{GPA}|C=N)
\end{eqnarray*} and predict $H$ for instances which solve the inequality corresponding to their AP status.
We solved the equivalent inqualities
\begin{equation}
    \ln \frac{p(AP=x_{AP} \mid C=H)}{p(AP=x_{AP} \mid C=N)}
    > \ln \frac{p(GPA=x_{GPA} \mid C=N)}{p(GPA=x_{GPA} \mid C=H)}
    \label{equation: interval constraint}
\end{equation}, both in cases where $x_{AP}=yes$ and $x_{AP}=no$.
Taking the logarithm reduces the problem to finding the roots of a quadratic.

Solving this inequality gives two GPA thresholds for predicting $H$ on an instance $x$ with AP feature $x_{AP}$. It defines the interval where you would predict that a student will be normal and produces the following prediction rule:

If AP courses are taken, predict H if the GPA is not between 1.99183 and 2.30188. \\
If AP courses are not taken, never predict H (unless the GPA can be somehow outside of range $[0,4.3]$).

In reality though it doesn't make sense that a student with a GPA between 0 and 1.99 would be an honors student if they simply took an AP class.
With this belief we can augment our prediction rule and only predict H on instances with GPA scores greater than the thresholds calculated from equation \eqnref{interval constraint}.
This would yield the following prediction rule:

If AP courses are taken, predict H if the GPA is above 2.30188. \\
If AP courses are not taken, never predict H (unless the GPA is over 4.3 somehow).
 
\section{Bayesian Decision Theory}

In this problem we are classifying automobiles.
Given an instance.
We can predict {\bf B}us, {\bf C}ar, or a {\bf T}ruck.
We may also choose to {\bf A}bstain (give no prediction).
The cost matrix for the decisions is
\begin{center}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        loss &
        \multicolumn{4}{|c|}{prediction} \\
        \multicolumn{1}{|r|} {class} & B & C & T & A \\
        \hline
        B & 0 & 1 & 1 & $a$ \\
        C & 1 & 0 & 1 & $a$ \\
        T & 1 & 1 & 0 & $a$ \\
        \hline
    \end{tabular}
\end{center}
where $a$ is a cost (or loss) incurred from abstaining from a prediction.
We will denote the loss of a prediction $\hat y$ given a true label $y$ as
$L(\hat{y} \mid y)$. For example, $L(B|C)=1, L(T|T) = 0 \text{ and } L(A|C)=a$.

\begin{abclist}
\item Assume that for an arbitrary instance $P(B) = 0.5, P(C) = 0.25, \text{ and } P(T) = 0.25$.
    The risk $R(\hat y)$ of any prediction $\hat y$ is 
    \begin{equation}
        R(\hat y) = E_y[L(\hat{y} \mid y)] = \sum_{y \in \left\{ B,C,T \right\}} P(y) L(\hat{y} \mid y)
        \label{equation: risk}
    \end{equation}.

    Equation \eqnref{risk} allows us to calculate the risk associated with each prediction (or lack there of)
    \begin{center}
        \begin{tabular}{|l|c|}
            \hline
            class & risk \\
        \hline
        B & 0.5 \\
            C & 0.75 \\
            T & 0.75 \\
            A & $a$ \\
            \hline
        \end{tabular}
    \end{center}.

\item A risk averse predictor would predict {\bf B} for any new instance if $a > 0.5 = R(B)$.
    They could predict {\bf B} or {\bf A}bstain with equal expectation for cost if $a = 0.5$.
    As $a$ tends to zero, the best predition for the risk averse predictor becomes to always abstain from giving a prediction, for which the cost tests to zero.
\end{abclist}


\section{Expectation of a product}
    First consider the following proposition.
    \begin{proposition}[Conditional expection of a product.]
        If $V$ and $W$ are random variables and $Z=VW$, then $E[Z|V=v,W=w]=\int_{-\infty}^{\infty}p(Z=z \mid V=v,W=w)dz = wv$.
        \label{proposition: conditional expectation}
    \end{proposition}
    This is true in either the continuous or discrete case.
    It is also a very straight forward and intuitive idea.
    To actually show it would require more in depth arguments involving the hairyness of the Dirac delta function for continuous variables.
    So here it is taken as fact.

    We can now discuss the main result of this problem.
\begin{lemma}[Product of idependent variables]
    Let $V$ and $W$ be independent random variables. Then $E[VW] = E[V]E[W]$.
\end{lemma}
\begin{proof}
    Assume $V$ and $W$ and independent random variables (numbers) and let $Z=VW$.
    %The joint probability density of $V$ and $Z$ is given as
    %\begin{equation}
    %p(V=v,Z=z) = p(V=v)p(Z=z|V=v) = p(V=v)p\bigg(W=\frac{z}{v}\bigg)
    %\end{equation}
    %Then the probability density of $Z$, $p(Z=z)$ is
    %\begin{eqnarray}
    %p(Z=z) &=& \int_{-\infty}^{\infty}p(V=v,Z=z)dv \\
    %&=& \int_{-\infty}^{\infty}p(V=v)p\bigg(W=\frac{z}{v}\bigg)dv
    %\label{equation: indep_prod_density}
    %\end{eqnarray}
    %The expected value of $Z$ is then
    %\begin{eqnarray}
    %E[Z] &=& \int_{-\infty}^{\infty} zp(Z=z) dz \nonumber \\
    %&=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}z p(V=\nu)p\bigg(W=\frac{z}{\nu}\bigg)d\nu dz \\
    %\label{equation: full_expected_indep_prod}
    %\end{eqnarray}
    %Making the change of variables $\nu = v, z = vw$, we get
    %\begin{eqnarray}
    %E[Z] &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
    %vwp(V=v)p(W=w) (v \text{ \color{red}{\dots shit}}) dv dw \\
    %&=& \int_{-\infty}^{\infty} wp(W=w) \int_{-\infty}^{\infty} v^2 p(V=v) dv dw \\
    %&=& E[W]E[V^2]
    %\end{eqnarray}
    %That extra $v$ term that popped out of the determinant of the Jacobian really throws things off.
    %I notice now that I never actually used the fact that $V$ and $W$ are independent.
    %This could be the key flaw in the proof, but I can't see where to apply it.
    %{\color{blue}Alternatively}, 
    
    %the conditional probability mass of $Z$ (discrete case) can be viewed as a simple indicator function.
    %\begin{equation}
    %p(Z=z|V=v,W=w) = {\bf 1}_{z=vw}
    %\label{equation: p z discrete}
    %\end{equation}
    %This should be obvious.
    %When the values of $V$ and $W$ are known there is no randomness left in $Z$.
    %Furthermore, it should be clear that the expected value of $Z$ given $V=v$ and $W=w$, $E[Z|V=v,W=w]$ is equal to $vw$.
    %
    %In the continuous case, the conditional density takes the form of a Dirac delta function shifted so all of $Z$'s density is centered over $vw$.
    %\begin{equation}
    %p(Z=z|V=v,W=w) = \delta(z - vw)
    %\label{equation: p z}
    %\end{equation}
    %More explicitly $p(Z=z|V=v,W=w)$ is equal to 0 for all $z \neq vw$, but  \[\int_{-\infty}^{\infty}p(Z=z|V=v,W=w)dz = \int_{-\infty}^{\infty}\delta(z-vw)dz = 1\].
    %The final observation about the delta function that we will make is that 
    %\begin{equation}
    %\int_{-\infty}^{\infty} f(z)\delta(z-vw)dz = f(vw)
    %\label{equation: delta func eval}
    %\end{equation}.
    %In particular, $E[Z] = \int_{-\infty}^{\infty} z\delta(z-vw)dz = vw$.
    
    The joint probability density of $V$, $W$, and $Z$ can be expanded using the chain rule of probability and our assumption of independence between $V$ and $W$.
    \begin{eqnarray}
        p(V=v, W=w, Z=z) &=& p(V=v, W=w)p(Z=z \mid V=v,W=w) \nonumber \\
            &=& p(V=v)p(W=w)p(Z=z \mid V=v,W=w)
            \label{equation: joint density}
    \end{eqnarray}.
    The probability density of $Z$ can then be marginalized from the join density.
    \begin{eqnarray}
        p(Z=z) &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(V=v, W=w, Z=z) dwdv \nonumber \\
        &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(V=v)p(W=w)p(Z=z \mid V=v,W=w) dwdv
        \label{equation: p z}
    \end{eqnarray}
    With proposition equation \eqnref{p z} and \propref{conditional expectation} we can calculate the expected value of $Z=VW$.
    \begin{eqnarray}
        E[Z] &=& \int_{-\infty}^{\infty} zp(Z=z)dz \nonumber \\
        &=& \int_{-\infty}^{\infty} z \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(V=v,W=w,Z=z) dwdvdz \nonumber \\
        &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} zp(V=v)p(W=w)p(Z=z \mid V=v,W=w) dwdvdz \nonumber \\
        &=& \int_{-\infty}^{\infty} p(V=v) \int_{-\infty}^{\infty} p(W=w) \int_{-\infty}^{\infty} zp(Z=z \mid V=v,W=w) dzdwdv \nonumber \\
        &=& \int_{-\infty}^{\infty} vp(V=v) \int_{-\infty}^{\infty} wp(W=w) dw dv = E[V]E[W]
        \label{equation: z_mean}
    \end{eqnarray}
    The order of integration is changed, which should be applicable in the discrete and continuous case\dots
\end{proof}

Let $R$ be a Bernoulli random variable equal to 1 with probablity 0.5, and equal to zero with probability 0.5.
Now, let $S=R$ and $T=RS$.
Note that $T$ has the same value as $R^2$.
Also, $R^2$ has the same value as $R$, since $R$ can only be 0 or 1.
So the random variable $T$ is identical to $R$.
So $E[T]=E[RS]=E[R^2]=E[R]=0.5$. This is not equal to $E[R]E[S]=E[R]^2=0.25$.




% Add a bibliography.
%\bibliographystyle{apalike}
%\bibliography{project}

%\end{multicols}



\end{document}
